---
title: "Modeling"
author: "Madalyn Young"
date: "2024-10-20"
output: html_document
---

```{r echo = FALSE, warning=FALSE}
pacman::p_load(caret, psych, rpart, rpart.plot, rJava, RWeka, rminer, matrixStats, knitr, tictoc, tidyverse, dplyr, ggplot2, randomForest)
```


```{r echo=FALSE}
App_train <- read.csv("home-credit-default-risk//application_train.csv") 
App_test <-  read.csv("home-credit-default-risk//application_test.csv") 
```

```{r}
#clean up App_Train
App_train <- App_train %>% 
  select(where(~ mean(is.na(.)) <= 0.57)) %>% 
  filter(AMT_INCOME_TOTAL <= 1000000) %>% 
  filter(CNT_CHILDREN <= 15) %>% 
  mutate(Age = -DAYS_BIRTH/365)

App_train$TARGET <- as.factor(App_train$TARGET)
```

```{r}
ggplot(App_train, aes(x=Age)) +
  geom_bar()
```


```{r}
scores <- App_train[,c("SK_ID_CURR", "EXT_SOURCE_1", "EXT_SOURCE_2","EXT_SOURCE_3")]

sum(is.na(scores$EXT_SOURCE_2))


App_train2 <- App_train %>%
  mutate(EXT_SOURCE_2 = ifelse(is.na(EXT_SOURCE_2), 
                               ifelse(is.na(EXT_SOURCE_3), EXT_SOURCE_1, 0), 
                               EXT_SOURCE_2))

sum(is.na(App_train2$EXT_SOURCE_2))

#172 IDs with missing credit
```
only 660 rows of nulls in the ext_source 2

He said the NA's can be predicitive. maybe we should use all three.

Most NAs in source 1. over half 

```{r}
#model 1
model <- lm(TARGET~EXT_SOURCE_1 + EXT_SOURCE_2 + EXT_SOURCE_3, data = App_train2)

summary(model)

#lm deletes the missing values. what happens if I replace nulls with 0?

# even when I replace with zero they are all significant

#leaderboard divided ext_source_3 with good results
  # you can divide total debt by the credit score to get debt burden relative to lift 

## We want to try this one!!!!!!
  # it can also scale the loan amounts if we divide it by loan amount (a large loan for someon with a low score might signal higher default)

#debt_credit_ratio_None: grouped by SK_ID_CURR, the sum of all credit debt (AMT_CREDIT_SUM_DEBT) over the sum of all credit (AM_CREDIT_SUM).



#we can create debt to income ratio or net work with income and credit columns
```


```{r}
#class he said the most basic is a logisitc reggession intercept only


model2 <- glm(target ~ 1,
              data = App_train,
              family = binomial)

summary(model2)
```
So start with the probability of default no predictors?


Create a binary variable with ALL ext_source variables for no credit history

DROP ANY COLUMNS WITH OVER 80% of VALUES MISSING
# Task 1
Start with glm, no predictors

# Task 2 

91.2%, what we have to beat in order to make the model useful. If we say yes we would be good 90% of the time 

# Task 3

Divide loan amount by credit score
  # it can also scale the loan amounts if we divide it by loan amount (a large loan for someon with a low score might signal higher default)
  
calculate debt to income ratio

interaction -> age*income, income*number of children 

```{r}
App_train
```

# Task 4 

use random forest

# Task 5

standardize = means are zero 
SVM requires us to make changes to categorical variables -> do an SVM for this task


# Task 1
**Set up a training set and a validation set using application_train.csv data set to do cross-validation.  Alternatively you could perform cross-validation using a different framework, such as k-fold cross validation as implemented in modeling packages such as caret or tidymodels or scikit-learn. The model performance that matters, of course, is the estimated performance on the test set as well as the Kaggle score.**

```{r}
set.seed(123)

inTrain <- createDataPartition(App_train$TARGET, p = .1, list = FALSE)

train_set <- App_train[inTrain,]
test_set <- App_train[-inTrain,]

test_target <- App_train[-inTrain,2]


```


# Task 2
**Identify the performance benchmark established by the majority class classifier.**

```{r}
prop.table(table(App_train$'TARGET'))
```


If we just went off the majority classifier to predict if someone would default, they would not 92% of the time. The performance benchmark is 92%.

# Task 3
**Fit several different logistic regression models using different predictors. Do interaction terms improve the model?  Compare model performance using not just accuracy but also AUC.**

```{r}
glmModel1 <- glm(TARGET~ 1, family = binomial, data = train_set)

summary(glmModel1)
```

```{r}
prob <- exp(coef(glmModel1)[1]) / (1 + exp(coef(glmModel1)[1]))
prob
```


The odds of the target being one are 0.079. The negative intercept shows tat there are low likelihood of the target being 1


```{r}
#is family = binomial correct?
glmModel2 <- glm(TARGET~Age + EXT_SOURCE_2+ AMT_INCOME_TOTAL, family = binomial, data = App_train)

summary(glmModel2)
```
The older you get the less likely you are to default

Total income is not significant in this model.

All these effects have a low impact

```{r}
glmModel2 <- glm(TARGET~Age + EXT_SOURCE_2+ AMT_INCOME_TOTAL*CNT_CHILDREN, family = binomial, data = App_train)

summary(glmModel2)
```
number of children and income are not significant. Their interaction is significant at the 95

```{r}

predictions <- predict(glmModel1, newdata = test_set)
summary(predictions)

##All thses are returning null???
mmetric(test_target, predictions, "ACC")


```

```{r}
all.vars(formula(glmModel2)) %in% names(test_set)

sum(is.na(test_set))

length(test_set)
```


NOT sure how to do AUC or ACC since the lengths of predictions and test_set are differnet and I am getting more predictions than in the test set and I have no idea why it is doing that. 

# Task 4
**Explore using algorithms like random forest and gradient boosting. Compare model performance.**

```{r}
train_setNA <- train_set %>% 
  select(TARGET, CNT_CHILDREN, AMT_INCOME_TOTAL, AMT_CREDIT, EXT_SOURCE_2, NAME_FAMILY_STATUS) %>% 
  na.omit()

rfModel1 <- randomForest(TARGET~ CNT_CHILDREN + AMT_INCOME_TOTAL + AMT_CREDIT + EXT_SOURCE_2 + NAME_FAMILY_STATUS,
                         data = train_setNA)

rfModel1
```

```{r}
rfModel1Preds <- predict(rfModel1, newdata = test_set)

summary(rfModel1Preds)

mmetric(test_target, rfModel1Preds, "ACC")
```
# Task 5
**Perform the data transformations required by a given algorithm.  For example, some algorithms require numeric data and perform better when it has been standardized or normalized. **

do svm
