---
title: "Modeling"
author: "Madalyn Young"
date: "2024-10-20"
output: html_document
---

```{r echo = FALSE, warning=FALSE}
pacman::p_load(caret, psych, rpart, rpart.plot, rJava, RWeka, rminer, matrixStats, knitr, tictoc, tidyverse, dplyr, ggplot2, randomForest, DMWR)
```


```{r echo=FALSE}
App_train <- read.csv("home-credit-default-risk//application_train.csv") 
App_test <-  read.csv("home-credit-default-risk//application_test.csv") 

App_train2 <- read.csv("home-credit-default-risk//application_train.csv") 
```

```{r}
#clean up App_Train
App_train <- App_train %>% 
  select(where(~ mean(is.na(.)) <= 0.57)) %>% 
  filter(AMT_INCOME_TOTAL <= 2000000) %>% #42 people default between 1M and 2M 
  filter(CNT_CHILDREN <= 15) %>% 
  mutate(Age = -DAYS_BIRTH/365)

App_train$TARGET <- as.factor(App_train$TARGET)
```

```{r}
#checking the outliers 
App_train2 %>% ggplot( aes(y=SK_ID_CURR, x=AMT_INCOME_TOTAL)) + geom_point()
```



```{r}

#checking how many missing values are in each ext source
scores <- App_train[,c("SK_ID_CURR", "EXT_SOURCE_1", "EXT_SOURCE_2","EXT_SOURCE_3")]

sum(is.na(scores$EXT_SOURCE_2))


App_train2 <- App_train %>%
  mutate(EXT_SOURCE_2 = ifelse(is.na(EXT_SOURCE_2), 
                               ifelse(is.na(EXT_SOURCE_3), EXT_SOURCE_1, 0), 
                               EXT_SOURCE_2))

sum(is.na(App_train2$EXT_SOURCE_2))

#172 IDs with missing credit
```
only 660 rows of nulls in the ext_source 3. Only 172 missing in 2

Most NAs in source 1. over half 



# Task 3

Divide loan amount by credit score
  # it can also scale the loan amounts if we divide it by loan amount (a large loan for someon with a low score might signal higher default)
  
calculate debt to income ratio

interaction -> age*income, income*number of children 





# Task 5

standardize = means are zero 
SVM requires us to make changes to categorical variables -> do an SVM for this task


# Task 1
**Set up a training set and a validation set using application_train.csv data set to do cross-validation.  Alternatively you could perform cross-validation using a different framework, such as k-fold cross validation as implemented in modeling packages such as caret or tidymodels or scikit-learn. The model performance that matters, of course, is the estimated performance on the test set as well as the Kaggle score.**

```{r}
set.seed(123)

#keep at 10% because the data set is large
inTrain <- createDataPartition(App_train$TARGET, p = .1, list = FALSE)

train_set <- App_train[inTrain,]
test_set <- App_train[-inTrain,]

test_target <- App_train[-inTrain,2]


```


# Task 2
**Identify the performance benchmark established by the majority class classifier.**

```{r}
prop.table(table(App_train$'TARGET'))
```


If we just went off the majority classifier to predict if someone would default, they would not 91.92% of the time. The performance benchmark is 91.92%.

# Task 3
**Fit several different logistic regression models using different predictors. Do interaction terms improve the model?  Compare model performance using not just accuracy but also AUC.**

```{r}
glmModel1 <- glm(TARGET~ 1, family = binomial, data = train_set)

summary(glmModel1)
```

```{r}
prob <- exp(coef(glmModel1)[1]) / (1 + exp(coef(glmModel1)[1]))
prob
```


The odds of the target being one are 0.079. The negative intercept shows that there are low likelihood of the target being 1

```{r}

predictions1 <- predict(glmModel1, newdata = test_set, type = "response")
summary(predictions1)

##All thses are returning null???
mmetric(test_target, predictions1, metric = c("ACC"))

```
Just using the target gets us to 91.93

```{r}
glmModel2 <- glm(TARGET~Age + EXT_SOURCE_1 + EXT_SOURCE_2 +  EXT_SOURCE_3 + AMT_INCOME_TOTAL + CNT_CHILDREN + DAYS_EMPLOYED + AMT_CREDIT + AMT_ANNUITY, family = binomial, data = train_set)

summary(glmModel2)
```
The older you get the less likely you are to default

Total income is not significant in this model.

All these effects have a low impact

```{r}
predictions2 <- predict(glmModel2, newdata = test_set, type = "response")
summary(predictions2)


predictions2 <- ifelse(predictions2>0.5,1,0)
confusionMatrix(factor(predictions2),factor(test_target))
```
The accuracy increased to 92.74. 

```{r}
#adding two interaction terms
#1 - age*amt_income
#2amt_income* cnt_children (not family members to stay consistent with the above)
glmModel3 <- glm(TARGET~ EXT_SOURCE_1 + EXT_SOURCE_2 + EXT_SOURCE_3 + Age*AMT_INCOME_TOTAL + AMT_INCOME_TOTAL*CNT_CHILDREN + DAYS_EMPLOYED + AMT_CREDIT + AMT_ANNUITY, family = binomial, data = train_set)

summary(glmModel3)
```
Interaction terms are not significant. The terms are not really significant on their own.



```{r}
predictions3 <- predict(glmModel3, newdata = test_set, type = "response")
summary(predictions3)


predictions3 <- ifelse(predictions3>0.5,1,0)
confusionMatrix(factor(predictions3),factor(test_target))
```
accuracy increases to 99.75, which is not much better. The interaction terms are not significant predictors on their own. So accuracy might be going up because there are more predictors and R2 is going up and therefore the model is overfitting.

###ADD AUC FROM IMOGEN CODE


# Task 4
**Explore using algorithms like random forest and gradient boosting. Compare model performance.**

```{r}

rfModel1 <- randomForest(TARGET~ Age+ EXT_SOURCE_1+ EXT_SOURCE_2+ EXT_SOURCE_3+ AMT_INCOME_TOTAL+ CNT_CHILDREN+ DAYS_EMPLOYED+  AMT_CREDIT+ AMT_ANNUITY,
                         data = train_set, ntree = 500, na.action = na.omit)

rfModel1
```

```{r}
rfModel1Preds <- predict(rfModel1, newdata = test_set, type = "response")

summary(rfModel1Preds)

confusionMatrix(rfModel1Preds,factor(test_target))
```

Accuracy is 92.69

BRING IN FEATURE IMPORTANCE IN THE HYPERPARAMETER

# Task 5
**Perform the data transformations required by a given algorithm.  For example, some algorithms require numeric data and perform better when it has been standardized or normalized. **

IMOGEN WILL DO

# Task 6 
**Experiment with upsampling and downsampling the data to adjust for the imbalanced target variable.  (See APM Ch. 16.)  Does this strategy this improve model performance?**

```{r}
upsampled_data <- upSample(x= train_set[,-2], y = train_set$TARGET, yname = 'TARGET')

#as.matrix(upsampled_data)
table(upsampled_data$TARGET)
```

```{r}
glmModelUP <- glm(TARGET~1, family = binomial, data = upsampled_data)

summary(glmModelUP)
```

Need to upsample with GLM2, and random forest

# Task 7
**Try combining model predictions--this is called an ensemble model--to improve performance.**

TYLER WILL DO

# Task 8
**Try additional feature engineering to boost model performance. Can you combine variables or bin numeric variables?  Explore the notebooks at Kaggle for data transformation ideas. In particular, use the other data sets at Kaggle--beyond the application data--to create additional features.**

TYLER WILL DO

# Task 9 
**For machine learning models experiment with hyperparameter tuning  to try to boost performance.**

```{r}
#we want to do hyperparameter tuning for random forest model -> notes under random forest
```

